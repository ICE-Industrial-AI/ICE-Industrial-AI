<!DOCTYPE html><html lang="en" data-theme="lofi"> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.10.1"><!-- Primary Meta Tags --><title>Review Zurich NLP #17</title><meta name="title" content="Review Zurich NLP #17"><meta name="description" content="Aleksander Ficek (NVIDIA) on synthetic generators &#38; verifiers for coding and Matteo Saponati (ETH Zurich) on the structures of self-attention beyond keys and queries."><!-- Open Graph / Facebook --><meta property="og:type" content="article"><meta property="og:url" content="https://astrofy-template.netlify.app/blog/review-zurich-nlp-17/"><meta property="og:title" content="Review Zurich NLP #17"><meta property="og:description" content="Aleksander Ficek (NVIDIA) on synthetic generators &#38; verifiers for coding and Matteo Saponati (ETH Zurich) on the structures of self-attention beyond keys and queries."><meta property="og:image" content="https://astrofy-template.netlify.app/zurichNLP.webp"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://astrofy-template.netlify.app/blog/review-zurich-nlp-17/"><meta property="twitter:title" content="Review Zurich NLP #17"><meta property="twitter:description" content="Aleksander Ficek (NVIDIA) on synthetic generators &#38; verifiers for coding and Matteo Saponati (ETH Zurich) on the structures of self-attention beyond keys and queries."><meta property="twitter:image" content="https://astrofy-template.netlify.app/zurichNLP.webp"><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><script type="module" src="/_astro/ClientRouter.astro_astro_type_script_index_0_lang.CtSceO8m.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"><link rel="stylesheet" href="/_astro/about.BXM3VkdQ.css">
<style>.time-line-container>div:last-child .education__time>.education__line{display:none}.astro-route-announcer{position:absolute;left:0;top:0;clip:rect(0 0 0 0);clip-path:inset(50%);overflow:hidden;white-space:nowrap;width:1px;height:1px}
</style></head> <body> <div class="bg-base-100 drawer lg:drawer-open"> <input id="my-drawer" type="checkbox" class="drawer-toggle"> <div class="drawer-content bg-base-100"> <div class="sticky lg:hidden top-0 z-30 flex h-16 w-full justify-center bg-opacity-90 backdrop-blur transition-all duration-100 bg-base-100 text-base-content shadow-sm"> <div class="navbar"> <div class="navbar-start"> <label for="my-drawer" class="btn btn-square btn-ghost"> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" class="inline-block w-5 h-5 stroke-current"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path> </svg> </label> </div> <div class="navbar-center"> <a class="btn btn-ghost normal-case text-xl" href="/">ICE Industrial-AI ğŸ§ ğŸ¤–</a> </div> <div class="navbar-end"></div> </div> </div> <div class="md:flex md:justify-center"> <main class="p-6 pt-10 lg:max-w-[900px] max-w-[100vw]">  <main class="md:flex md:justify-center"> <article class="prose prose-lg max-w-[750px] prose-img:mx-auto"> <img src="/zurichNLP.webp" alt="Review Zurich NLP #17" loading="lazy" decoding="async" fetchpriority="auto" width="750" height="422" class="w-full mb-6"> <h1 class="title my-2 text-4xl font-bold">Review Zurich NLP #17</h1> <time>Jun 24, 2025</time> <br> <div class="badge badge-secondary my-1">Latest</div> <a href="/blog/tag/zurichNLP" class="badge badge-outline ml-2 no-underline"> zurichNLP </a><a href="/blog/tag/LLM" class="badge badge-outline ml-2 no-underline"> LLM </a><a href="/blog/tag/Nvidia" class="badge badge-outline ml-2 no-underline"> Nvidia </a>  <div class="divider my-2"></div>  <h1 id="review-zurichnlp-17">Review ZurichNLP #17</h1>
<p><em>Author: Philipp GÃ©rard TrÃ©muel</em></p>
<h2 id="about-the-event">About the Event</h2>
<p>Aleksander Ficek (NVIDIA): Synthetic Generators and Verifiers for Coding
TBD</p>
<p>Matteo Saponati (ETH Zurich): Structure of Self-Attention Beyond Queries and Keys</p>
<p>Self-attention is essential to Transformer architectures, yet how information is embedded in the self-attention matrices and how different objective functions impact this process remains unclear. We present a mathematical framework to analyze self-attention matrices by deriving the structures governing their weight updates. Using this framework, we demonstrate that bidirectional training induces symmetry in the weight matrices, while autoregressive training results in directionality and column dominance. Our theoretical findings are validated across multiple Transformer models - including ModernBERT, GPT, LLaMA3, and Mistral - and input modalities like text, vision, and audio. Finally, we apply these insights by showing that symmetric initialization improves the performance of encoder-only models on language tasks. This mathematical analysis offers a novel theoretical perspective on how information is embedded through self-attention, thereby improving the interpretability of Transformerâ€¦</p>
<h2 id="-synthetic-data-for-code-and-reasoning-in-llms">ğŸ§  Synthetic Data for Code and Reasoning in LLMs</h2>
<p><em>Aleksander Ficek (NVIDIA)</em></p>
<h3 id="-the-rise-of-reasoning-models">ğŸš€ The Rise of Reasoning Models</h3>
<p>The field of LLMs entered a reasoning-driven renaissance at the end of 2024, ignited by <a href="https://openai.com/de-DE/index/learning-to-reason-with-llms/">OpenAIâ€™s O1</a> in December 2024 and followed by DeepSeek-R1 in January 2025. These models showcased the next step of beyond <a href="https://arxiv.org/abs/2201.11903">chain-of-thought</a> (CoT) reasoning and how breaking down complex problems step by step could be embedded into training, not just inference by using Reinforcement Learning (RL).</p>
<p>Yet, contrary to earlier belief that RL was essential for state-of-the-art reasoning, <a href="https://arxiv.org/abs/2504.01943">OpenCodeReasoning</a> has shown that <strong>supervised fine-tuning (SFT)</strong> aloneâ€”on a large synthetic datasetâ€”can achieve comparable results on coding benchmark.</p>
<h3 id="ï¸-opencodereasoning-sft-only-reasoning-mastery">âš™ï¸ OpenCodeReasoning: SFT-Only Reasoning Mastery</h3>
<ul>
<li><strong>Dataset Scale &#x26; Method</strong>
<ul>
<li>Over <strong>736K Python code solutions</strong> with full reasoning traces, covering <strong>28,900+ unique</strong> competitive programming tasks.</li>
<li>Solutions are generated by a strong teacher model (like DeepSeekâ€‘R1), then validated via unit tests and distilled into student models.</li>
</ul>
</li>
<li><strong>Performance Highlights</strong>
<ul>
<li>Qwen2.5 fined-tuned with this dataset hits <strong>pass@1 rates of 51.3% (7B)</strong>, <strong>59.4% (14B)</strong>, and <strong>61.8% (32B)</strong> on LiveCodeBenchâ€”beating comparable SFT-only baselines and matching DeepSeekâ€‘R1-distilled versions.</li>
<li>Surpasses reinforcementâ€‘learning-enhanced models on code tasks, validating pure SFT scaling.</li>
</ul>
</li>
<li>Insights
<ul>
<li>Scaling SFT data yields steady performance gainsâ€”no evidence of plateauing.</li>
<li>Surprisingly, including some incorrect solutions alongside unit-tests in the dataset boosts model robustness on hard problems.</li>
</ul>
</li>
<li>Limitations
<ul>
<li>Pure SFT lacks internal self-correction: models often stick to their own faulty answers .</li>
<li>Improving via SFT-only might hit a ceilingâ€”hence NVIDIAâ€™s next step.</li>
</ul>
</li>
</ul>
<h3 id="-nvidias-llama-nemotron-sft--rl--efficiency--power">ğŸ¯ NVIDIAâ€™s Llama-Nemotron: SFT + RL = Efficiency &#x26; Power</h3>
<p><img src="/src/content/blog/imgs_post3/llama_nemotron1.png" alt="Figure 1">
<em>Figure 1 | As of April 2025, our flagship model LN-Ultra is the most â€œintelligentâ€ open model according to Artificial Analysis.</em></p>
<ol>
<li>
<p><strong><a href="https://www.nvidia.com/de-de/ai-data-science/foundation-models/nemotron/">Llamaâ€‘Nemotron</a> (Nano 8B, Super 49B, Ultra 253B) [<a href="https://arxiv.org/abs/2505.00949">Paper</a>]</strong></p>
<p>Try <a href="https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct">Llama-3_1-nemotron-70b-instruct</a></p>
<ul>
<li>Heterogeneous models optimized via <strong>Neural Architecture Search (NAS)</strong> for faster inference and better memory use.</li>
<li>Support a <strong>dynamic â€œreasoning toggleâ€</strong>â€”users can switch detailed reasoning on/off in real time.</li>
<li>Trained in stages:
<ul>
<li>Architecture optimization + distillation + continued pretraining</li>
<li><strong>Supervised fine-tuning</strong> on reasoning tasks (including traces from DeepSeek-R1)</li>
<li><strong>Large-scale reinforcement learning</strong> on STEM datasets</li>
<li>Short human-alignment tuning.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Performance</strong></p>
<ul>
<li>Ultraâ€‘253B outperforms DeepSeekâ€‘R1 on many reasoning benchmarks, while maintaining <strong>4Ã— higher throughput</strong> and fitting on a single 8â€‘GPU node.</li>
<li>Hits <strong>76% on GPQA-Diamond</strong> (PhD-level science reasoning!?), compared to 65% for humans.</li>
</ul>
</li>
</ol>
<p><img src="/src/content/blog/imgs_post3/llama_nemotron2.png" alt="Figure 2">
<em>Figure 2 | LN-Ultra delivers leading performance among open models across a wide range of reasoning and non-reasoning benchmarks.</em></p>
<ol>
<li><strong>Open &#x26; Productionâ€‘Ready</strong>
<ul>
<li>All models, weights, datasets, and tools (NeMo, Megatronâ€‘LM, NeMoâ€‘Aligner) are open under NVIDIAâ€™s permissive license.</li>
<li>Available via NIM microservices and Hugging Face, integrated across cloud and enterprise AI platforms</li>
</ul>
</li>
</ol>
<h3 id="-sft-only-vssftrl-trade-offs--challenges">ğŸ” SFT-only vsâ€¯SFT+RL: Trade-offs &#x26; Challenges</h3>








































<table><thead><tr><th>Aspect</th><th>SFT-Only (OpenCodeReasoning)</th><th>SFT+RL (Nemotron)</th></tr></thead><tbody><tr><td><strong>Performance (Code)</strong></td><td>â‰ˆ61.8% pass@1 (32B)</td><td>Nemotron Super/Ultra matches or exceeds SFT-only</td></tr><tr><td><strong>Reasoning Selfâ€‘check</strong></td><td>Limited; often fails on self-reflection â†’ observation from the NVIDIA researchers</td><td>RL enhances quality and internal error correction. As mentioned in the talk, this is reached by using an external LLM judge</td></tr><tr><td><strong>Inference Efficiency</strong></td><td>Depends on architecture</td><td>Optimized via NAS, offering 4â€“5Ã— faster throughput</td></tr><tr><td><strong>Training Complexity</strong></td><td>Simpler, stable, reproducible (SFT)</td><td>Harder: RL, reward design, stability issues</td></tr><tr><td><strong>Resource Requirements</strong></td><td>Lower; one stage SFT</td><td>Multi-stage: SFT + largeâ€‘scale RL</td></tr><tr><td><strong>Adaptability</strong></td><td>Strong for coding; limited beyond</td><td>Excels in math, science, coding, tool-use</td></tr></tbody></table>
<h3 id="-reflections-where-do-we-go-from-here-personal-thoughs">ğŸ¤” Reflections: Where Do We Go from Here? (Personal Thoughs)</h3>
<ul>
<li><strong>Blending SFT with targeted RL</strong> (as done by Nemotron) yields powerful, efficient models with improved reasoning and error correction.</li>
<li>The approach of using an LLM-Judge is similar to a gameâ€‘theoretical setup, like a consensus or competitive game. The stability and fairness between <strong>Generator (model)</strong> and <strong>Judge (RL objective)</strong> can be critical.
<ul>
<li>How to ensure stable training and reach an equilibrium like in training GANs?</li>
<li>How to ensure the Judge isnâ€™t hallucinating?</li>
</ul>
</li>
<li>In my opinion, an external â€œevaluatorâ€ like a code execution engine or unit tests is still required. Maybe they use the Judge to define the unit tests.</li>
</ul>
<h3 id="-in-conclusion">ğŸ§­ In Conclusion</h3>
<ul>
<li><strong>OpenCodeReasoning</strong> has redefined how far pure SFT can go by scaling synthetic reasoning datasets that can rival RL-enhanced approaches in coding benchmarks.</li>
<li><strong>NVIDIAâ€™s Llama-Nemotron</strong>, by integrating SFT with large-scale RL and architecturally efficient designs, raises the bar further, delivering fast, high-accuracy reasoning in real-world formats.</li>
<li>The quest ahead? Developing models that arenâ€™t just smart, but truly inventive and trustworthy, capable of generating novel, correct solutions.</li>
</ul>
<hr>
<h2 id="-the-underlying-structures-of-self-attention-symmetry-directionality--emergent-dynamics">ğŸ§  The Underlying Structures of Self-Attention: Symmetry, Directionality &#x26; Emergent Dynamics</h2>
<p><em>Matteo Saponati (ETH Zurich)</em></p>
<p>The paper â€œ<a href="https://arxiv.org/abs/2502.10927">The underlying structures of self-attention: symmetry, directionality, and emergent dynamics in Transformer training</a>â€  dives deep into the often-overlooked geometry of the Qâ€“K weight products in self-attention, revealing how different training objectives sculpt their fundamental structureâ€”and even how those structures can be harnessed to improve model performance.</p>
<h3 id="-what-problem-are-they-tackling">ğŸ” What Problem Are They Tackling?</h3>
<p>While self-attention was a breakthrough in Transformers by mapping queries and keys (via <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">W_Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">W_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>â€‹â€‹) into attention patterns via <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> itâ€™s still not well understood why it works so well. This paper builds a mathematical framework that connects gradient dynamics to specific structural patterns in the joint query-key matrix (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>Q</mi><mi>K</mi></mrow></msub><mo>=</mo><msubsup><mi>W</mi><mi>Q</mi><mi>T</mi></msubsup><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">W_{QK}=W_Q^TW_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2528em;vertical-align:-0.4114em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4247em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.4114em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> â€‹, also referred to as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>q</mi><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{qk}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>) .</p>
<p><img src="/src/content/blog/imgs_post3/self_att1.JPG" alt="Figure 3">
<em>Figure 3 | Illustration of the computation of the self-attention score between token <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and token <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">x_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>.</em></p>
<h3 id="-symmetry-vs-directionality-the-core-insights">ğŸ§© Symmetry vs. Directionality: The Core Insights</h3>
<ol>
<li><strong>Autoregressive (decoder-only) training</strong>
<ul>
<li>Predicts each token from the past only.</li>
<li>The resulting updates to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>q</mi><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{qk}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>â€‹ are asymmetric and create column dominance, introducing a learned directionality in the matrix.</li>
</ul>
</li>
<li><strong>Bidirectional (encoder-only) training</strong>
<ul>
<li>Enables tokens to condition on both past and future context.</li>
<li>This results in symmetric weight updates, leading to highly symmetric <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>q</mi><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{qk}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>â€‹ matrices.</li>
</ul>
</li>
</ol>
<p>These differences are not just theoreticalâ€”theyâ€™re measurable.</p>
<p><img src="/src/content/blog/imgs_post3/self_att2.JPG" alt="Figure 4">
<em>Figure 4 | a) Left) Median symmetry score of the matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>q</mi><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{qk}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> as a function of the total number of parameters.
Each dot corresponds to the median and the interquartile range across layers of a given pre-trained model (see
Tables in Appendix S5). Right) Example of structures in the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>q</mi><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{qk}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> matrix of an encoder-only model (BERT
Tiny, layer 1 [<a href="https://arxiv.org/abs/1908.08962">Turc et al., 2019</a>]) b) Left) Same as in a for the median directionality score of the matrix Wqk.
Right) Example of structures in the Wqk matrix of a decoder-only model (TinyStories GPT, layer 1 [<a href="https://arxiv.org/abs/2305.07759">Eldan and Li, 2023</a>])</em></p>
<h3 id="-experiments-confirm-theory">ğŸ“Š Experiments Confirm Theory</h3>
<ul>
<li>The authors introduced symmetry and directionality scores to quantify structural biases in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>q</mi><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{qk}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>â€‹.</li>
<li>Evaluated across multiple models (e.g., BERT, GPT, LLaMA3, Mistral) and even audio/vision modalities, they consistently found:
<ul>
<li>Encoder-only models â†’ high symmetry scores</li>
<li>Decoder-only models â†’ high directionality scores</li>
</ul>
</li>
</ul>
<p>They further tracked how these scores evolve during trainingâ€”symmetry steadily grows in encoder models, while directionality becomes pronounced in decoder models .</p>
<h3 id="ï¸-practical-payoff-symmetric-initialization">âš™ï¸ Practical Payoff: Symmetric Initialization</h3>
<p>Inspired by the theory, they experiment with enforcing symmetry in initialization for encoder-only models. The result?</p>
<p>âœ… Faster convergence and better final performance on language tasksâ€”demonstrating a clear practical benefit.</p>
<h3 id="-correcting-common-misconceptions">ğŸ“ Correcting Common Misconceptions</h3>
<p>On QK math: Yes, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>q</mi><mi>k</mi></mrow></msub><mo>=</mo><msubsup><mi>W</mi><mi>Q</mi><mi>T</mi></msubsup><msub><mi>W</mi><mi>K</mi></msub><mtext>â€‹</mtext></mrow><annotation encoding="application/x-tex">W_{qk} = W_Q^TW_Kâ€‹</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2528em;vertical-align:-0.4114em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4247em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.4114em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">â€‹</span></span></span></span>â€‹. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> are separate projections; they donâ€™t multiply as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msub><mi>W</mi><mi>Q</mi></msub><msub><mi>W</mi><mi>K</mi></msub><mi>K</mi></mrow><annotation encoding="application/x-tex">QW_QW_KK</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>, but rather as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub><mi>Q</mi><mo>=</mo><mi>X</mi><mi>W</mi><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q=XW_QQ=XWQ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">Q</span></span></span></span>â€‹, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">K=XW_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and then <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> forms the bilinear attention.</p>
<ul>
<li><strong>On directional models</strong>: Autoregressive models will show early-token bias, but the structural phenomena revealed are broaderâ€”they reflect the gradient dynamics, not just inference behavior.</li>
<li><strong>On impact scope:</strong> While the paper doesnâ€™t propose a new LLM architecture, it illuminates the internal mechanisms of self-attention, offering <strong>interpretability</strong> and <strong>practical optimization opportunities</strong>.</li>
</ul>
<h3 id="-why-this-matters">ğŸš€ Why This Matters</h3>
<ul>
<li><strong>Theoretically rich:</strong> Bridges Transformer updates with bilinear algebra.</li>
<li><strong>Empirically solid:</strong> Consistent across architectures and data modalities.</li>
<li><strong>Pragmatically useful:</strong> Simple symmetric init yields measurable gains.</li>
<li><strong>Interpretability boost:</strong> Helps us â€œpeek under the hoodâ€ of self-attention dynamics.</li>
</ul>
<h3 id="-takeaways-for-researchers--practitioners">ğŸ“Œ Takeaways for Researchers &#x26; Practitioners</h3>
<ul>
<li>Be conscious: encoder-only vs decoder-only training induces <strong>different geometric biases</strong> in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>q</mi><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{qk}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>â€‹.</li>
<li>Consider <strong>symmetric initialization</strong> in encoder settingsâ€”it may enhance convergence.</li>
<li>Use the symmetry/directionality scores as tools to <strong>analyze internal structure</strong> during model diagnostics.</li>
</ul>
<p>This paper offers a mathematically grounded, empirically validated lens into how <strong>training objectives shape self-attention dynamics</strong>â€”revealing symmetry in bidirectional models and directionality in autoregressive ones. The icing on the cake: initializing self-attention weight matrices symmetrically actually helps. Not revolutionary, but a meaningful step forward in understanding and improving Transformers ğŸ”§.</p>  </article> </main>  </main> </div> <footer class="footer footer-center block mb-5 pt-10"> <div class="pb-2">
&copy; 2025 Industrial-AI
</div> <div class="inline opacity-75">
Developed by <a href="https://manuelernestog.github.io" target="_blank" class="font-bold">ICE Industrial-AI</a> using
<a href="https://astrofy-template.netlify.app/" target="_blank" class="font-bold">Astrofy Template âš¡ï¸</a> </div> </footer>  </div> <div class="drawer-side z-40"> <label for="my-drawer" class="drawer-overlay"></label> <aside class="px-2 pt-2 h-auto min-h-full w-[19rem] bg-base-200 text-base-content flex flex-col"> <div class="w-fit mx-auto mt-5 mb-6"> <a href="/"> <div class="avatar transition ease-in-out hover:scale-[102%] block m-auto"> <div class="w-[8.5rem]"> <img src="/profile.webp" alt="Profile image" loading="lazy" decoding="async" fetchpriority="auto" width="300" height="300" class="mask mask-circle"> </div> </div> </a> <div class="text-center mt-4"> <h2 class="text-lg font-semibold text-base-content">ICE Industrial-AI</h2> </div> </div> <ul class="menu grow shrink menu-md overflow-y-auto"> <li><a class="py-3 text-base" id="home" href="/">Home</a></li> <li><a class="py-3 text-base" id="about" href="/about">About Us</a></li> <li><a class="py-3 text-base" id="projects" href="/projects">Projects</a></li> <li><a class="py-3 text-base" id="stammtisch" href="/events">Events/AI-Stammtisch</a></li> <li><a class="py-3 text-base" id="blog" href="/blog/">Blog</a></li> <li><a class="py-3 text-base" id="publications" href="/publications">Publications</a></li> <li> <a class="py-3 text-base" href="mailto:ice@ost.ch" target="_blank" referrerpolicy="no-referrer-when-downgrade">Contact</a> </li> </ul> <script>(function(){const sideBarActiveItemID = undefined;
const activeClass = "bg-base-300";

const activeItemElem = document.getElementById(sideBarActiveItemID);
activeItemElem && activeItemElem.classList.add(activeClass);
})();</script> <div class="block sticky pointer-events-none bottom-10 bg-base-200 justify-center h-12 [mask-image:linear-gradient(transparent,#000000)]"></div> <div class="social-icons px-4 pb-5 pt-1 flex self-center justify-center sticky bottom-0 bg-base-200"> <a href="" target="_blank" class="mx-3" aria-label="Github" title="Github"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" style="fill: currentColor;transform: ;msFilter:;"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.026 2c-5.509 0-9.974 4.465-9.974 9.974 0 4.406 2.857 8.145 6.821 9.465.499.09.679-.217.679-.481 0-.237-.008-.865-.011-1.696-2.775.602-3.361-1.338-3.361-1.338-.452-1.152-1.107-1.459-1.107-1.459-.905-.619.069-.605.069-.605 1.002.07 1.527 1.028 1.527 1.028.89 1.524 2.336 1.084 2.902.829.091-.645.351-1.085.635-1.334-2.214-.251-4.542-1.107-4.542-4.93 0-1.087.389-1.979 1.024-2.675-.101-.253-.446-1.268.099-2.64 0 0 .837-.269 2.742 1.021a9.582 9.582 0 0 1 2.496-.336 9.554 9.554 0 0 1 2.496.336c1.906-1.291 2.742-1.021 2.742-1.021.545 1.372.203 2.387.099 2.64.64.696 1.024 1.587 1.024 2.675 0 3.833-2.33 4.675-4.552 4.922.355.308.675.916.675 1.846 0 1.334-.012 2.41-.012 2.737 0 .267.178.577.687.479C19.146 20.115 22 16.379 22 11.974 22 6.465 17.535 2 12.026 2z"></path> </svg> </a> <a href="" target="_blank" class="mx-3" aria-label="Twitter" title="Twitter"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" style="fill: currentColor;transform: ;msFilter:;"><path d="M19.633 7.997c.013.175.013.349.013.523 0 5.325-4.053 11.461-11.46 11.461-2.282 0-4.402-.661-6.186-1.809.324.037.636.05.973.05a8.07 8.07 0 0 0 5.001-1.721 4.036 4.036 0 0 1-3.767-2.793c.249.037.499.062.761.062.361 0 .724-.05 1.061-.137a4.027 4.027 0 0 1-3.23-3.953v-.05c.537.299 1.16.486 1.82.511a4.022 4.022 0 0 1-1.796-3.354c0-.748.199-1.434.548-2.032a11.457 11.457 0 0 0 8.306 4.215c-.062-.3-.1-.611-.1-.923a4.026 4.026 0 0 1 4.028-4.028c1.16 0 2.207.486 2.943 1.272a7.957 7.957 0 0 0 2.556-.973 4.02 4.02 0 0 1-1.771 2.22 8.073 8.073 0 0 0 2.319-.624 8.645 8.645 0 0 1-2.019 2.083z"></path> </svg> </a> <a href="https://www.linkedin.com/showcase/ost-ice-institut-f%C3%BCr-computational-engineering/?viewAsMember=true" target="_blank" class="mx-3" aria-label="Linkedin" title="Linkedin"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" style="fill: currentColor;transform: ;msFilter:;"><circle cx="4.983" cy="5.009" r="2.188"></circle><path d="M9.237 8.855v12.139h3.769v-6.003c0-1.584.298-3.118 2.262-3.118 1.937 0 1.961 1.811 1.961 3.218v5.904H21v-6.657c0-3.27-.704-5.783-4.526-5.783-1.835 0-3.065 1.007-3.568 1.96h-.051v-1.66H9.237zm-6.142 0H6.87v12.139H3.095z"></path> </svg> </a> <a href="" target="_blank" class="mx-3" aria-label="RSS Feed" title="RSS Feed"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" style="fill: currentColor;transform: ;msFilter:;"><path d="M19 20.001C19 11.729 12.271 5 4 5v2c7.168 0 13 5.832 13 13.001h2z"></path><path d="M12 20.001h2C14 14.486 9.514 10 4 10v2c4.411 0 8 3.589 8 8.001z"></path><circle cx="6" cy="18" r="2"></circle> </svg> </a> </div> </aside> </div> </div> </body></html>